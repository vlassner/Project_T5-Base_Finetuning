{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V28"},"accelerator":"TPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Learning Project\nBy Victoria Lassner\nDSML 4220\n\n**Goal**: Fine tune a model for abstractive Summarization.\n\n**Model:** T5-Base with its Tokenizer\n\nWebsites: https://huggingface.co/docs/transformers/tasks/summarization\n\n**Future Models to Compare:**\n\nhttps://wandb.ai/mostafaibrahim17/ml-articles/reports/Fine-Tuning-LLaMa-2-for-Text-Summarization--Vmlldzo2NjA1OTAy\n\nhttps://wandb.ai/mostafaibrahim17/ml-articles/reports/Crafting-Superior-Summaries-The-ChatGPT-Fine-Tuning-Guide--Vmlldzo1Njc5NDI1\n\n**Definitions:**\n\nAbstractive summarization = oncise summary of a text by understanding its meaning and creating new sentences, rather than simply extracting phrases from the original text.\n\n*****\n**Dataset:**\nCNN/DailyMail: https://paperswithcode.com/dataset/cnn-daily-mail-1\nBillSum\n","metadata":{"id":"xip9q78PK-Sh"}},{"cell_type":"code","source":"# disables weights and biases\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"id":"VCQVE40eIC9u","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:48:37.632221Z","iopub.execute_input":"2025-05-03T15:48:37.632969Z","iopub.status.idle":"2025-05-03T15:48:37.639275Z","shell.execute_reply.started":"2025-05-03T15:48:37.632915Z","shell.execute_reply":"2025-05-03T15:48:37.638614Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# downloads packages for model, dataset and tokenzier\n# --Quiet limits output of messages\n!pip install transformers datasets sentencepiece --quiet\n!pip install -q huggingface_hub transformers datasets","metadata":{"id":"BRJlkbTc-AHe","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:48:37.640451Z","iopub.execute_input":"2025-05-03T15:48:37.640695Z","iopub.status.idle":"2025-05-03T15:48:45.230650Z","shell.execute_reply.started":"2025-05-03T15:48:37.640679Z","shell.execute_reply":"2025-05-03T15:48:45.229515Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Download packages\nfrom datasets import load_dataset, concatenate_datasets\nfrom transformers import T5ForConditionalGeneration, TrainingArguments, Trainer, T5Tokenizer\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch","metadata":{"id":"Rl1bQHrq-FUW","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:48:45.232496Z","iopub.execute_input":"2025-05-03T15:48:45.232825Z","iopub.status.idle":"2025-05-03T15:49:09.633097Z","shell.execute_reply.started":"2025-05-03T15:48:45.232792Z","shell.execute_reply":"2025-05-03T15:49:09.632512Z"}},"outputs":[{"name":"stderr","text":"2025-05-03 15:48:57.877781: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746287338.032346      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746287338.078950      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load CNN/Daily Mail Dataset from dataset package\n# Limit samples to 4000 total.\n\ntrain_sample_limit = 3000\nval_sample_limit = 1000\n\ndataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\nlimited_train_data = dataset[\"train\"].select(range(train_sample_limit))\nlimited_val_data = dataset[\"validation\"].select(range(val_sample_limit))\n","metadata":{"id":"1T3geNBfK9f4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7e594d97-273f-4c3e-b7ff-4f8133f84989","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:49:09.633838Z","iopub.execute_input":"2025-05-03T15:49:09.634464Z","iopub.status.idle":"2025-05-03T15:49:32.391740Z","shell.execute_reply.started":"2025-05-03T15:49:09.634441Z","shell.execute_reply":"2025-05-03T15:49:32.391051Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce5219411a954d94b63f64ccde6a3c06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6db5ec288db4013af4bccd2dc2ea75d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b66d2cfd6d974fa18f028d91da68a17a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e85899ba3a0a4c46a1d6231ffc796c72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd9f1bbc4be2478189b408040a5781bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9119cc9b2d4429495132d4b2dbcda1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b90092c469e24c2db4e7c88c4966dee6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c43208ffaf294c98b908fcc284af9857"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a23338a3fbe4dff8b3867b85042a644"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# preprocess data for model\ntokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n\n# limit length of input articles and output summary\nmax_input_length = 512\nmax_target_length = 150\n\nchunk_size = 1000\n\ndef preprocess(examples):\n    inputs = [\"summarize: \" + doc for doc in examples[\"article\"]]\n    targets = examples[\"highlights\"]\n\n    model_inputs = tokenizer(\n        inputs,\n        max_length=max_input_length,\n        truncation=True,\n        padding=\"max_length\"\n    )\n\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets,\n            max_length=max_target_length,\n            truncation=True,\n            padding=\"max_length\"\n        )\n\n    model_inputs[\"labels\"] =labels[\"input_ids\"]\n    return model_inputs\n\n\ndef process_in_chunks(dataset, chunk_size, preprocess_fn):\n    total_len = len(dataset)\n    processed_chunks = []\n\n    for i in range(0, total_len, chunk_size):\n        chunk = dataset.select(range(i, min(i + chunk_size, total_len)))\n        processed_chunk = chunk.map(\n            preprocess_fn,\n            batched=True,\n            remove_columns=[\"article\", \"highlights\", \"id\"]\n        )\n        processed_chunks.append(processed_chunk)\n\n    return concatenate_datasets(processed_chunks)\n\n# Process the training and validation data into chunks\ntrain_dataset = process_in_chunks(limited_train_data, chunk_size, preprocess)\nval_dataset = process_in_chunks(limited_val_data, chunk_size, preprocess)","metadata":{"id":"0RKMbK6V-Z6p","colab":{"base_uri":"https://localhost:8080/"},"outputId":"23bd582d-f896-4a24-8126-15ad86df3e12","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:49:32.393502Z","iopub.execute_input":"2025-05-03T15:49:32.393771Z","iopub.status.idle":"2025-05-03T15:49:49.163622Z","shell.execute_reply.started":"2025-05-03T15:49:32.393753Z","shell.execute_reply":"2025-05-03T15:49:49.162964Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8369c7bdfe2741aaa63aa57da880fa33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8120f64e4eb4ae999a5af3ff231a1bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11b326108dde4dc79ef8c509e78c3f88"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb0362e47ad2439dacf8c7a479d67c22"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eab902279a424a769d767550a6f8c145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38e0ba6a147a403287c3e2e7b16b9fcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75e08fa5e13f43188d7df5a05b004daa"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Load model T5-base\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./t5-cnn-checkpoints\",\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,\n    num_train_epochs=5,\n    save_steps=1000,\n    logging_dir='./logs',\n    logging_steps=50,\n    save_total_limit=2,\n    fp16=torch.cuda.is_available(),\n)\n","metadata":{"id":"WcZyeG39-MAa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e5934b22-cae1-4231-8256-579f8fa727ea","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:49:49.164243Z","iopub.execute_input":"2025-05-03T15:49:49.164464Z","iopub.status.idle":"2025-05-03T15:49:58.166349Z","shell.execute_reply.started":"2025-05-03T15:49:49.164446Z","shell.execute_reply":"2025-05-03T15:49:58.165833Z"}},"outputs":[{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83b51a4341e44a12a86e68a96f7b3a69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3df93447bba41e1ab5cf58e24603ef9"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# adds padding so shorter sequences match the longest one\nfrom transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)","metadata":{"id":"Jg0RKsVhIsbu","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:49:58.167154Z","iopub.execute_input":"2025-05-03T15:49:58.167377Z","iopub.status.idle":"2025-05-03T15:49:58.171234Z","shell.execute_reply.started":"2025-05-03T15:49:58.167360Z","shell.execute_reply":"2025-05-03T15:49:58.170501Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# train model using hugging face's trainer class\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=data_collator,\n)\n\ntrainer.train()\n\ntrainer.evaluate()","metadata":{"id":"5gYF94aB-PDD","colab":{"base_uri":"https://localhost:8080/","height":113},"outputId":"f41a8ed6-0c44-42e7-80fa-fa172a02f0cf","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T15:49:58.172079Z","iopub.execute_input":"2025-05-03T15:49:58.172285Z","iopub.status.idle":"2025-05-03T16:19:25.487945Z","shell.execute_reply.started":"2025-05-03T15:49:58.172265Z","shell.execute_reply":"2025-05-03T16:19:25.487334Z"}},"outputs":[{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='465' max='465' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [465/465 28:32, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.904700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.738500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.686200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.665100</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.656600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.646900</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.635600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.626400</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.639100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 00:47]\n    </div>\n    "},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.6198398470878601,\n 'eval_runtime': 48.3505,\n 'eval_samples_per_second': 20.682,\n 'eval_steps_per_second': 2.585,\n 'epoch': 4.949333333333334}"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"#saves current state of model and tokenzier\nmodel.save_pretrained(\"/content/t5_cnn_model_base_v2\")\ntokenizer.save_pretrained(\"/content/t5_cnn_model_base_v2\")","metadata":{"id":"8tsgE2V8-Q6d","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:19:25.488617Z","iopub.execute_input":"2025-05-03T16:19:25.488814Z","iopub.status.idle":"2025-05-03T16:19:27.373055Z","shell.execute_reply.started":"2025-05-03T16:19:25.488799Z","shell.execute_reply":"2025-05-03T16:19:27.372445Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"('/content/t5_cnn_model_base_v2/tokenizer_config.json',\n '/content/t5_cnn_model_base_v2/special_tokens_map.json',\n '/content/t5_cnn_model_base_v2/spiece.model',\n '/content/t5_cnn_model_base_v2/added_tokens.json')"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\n# Load model and tokenizer\nmodel = T5ForConditionalGeneration.from_pretrained(\"/content/t5_cnn_model_base_v2\")\ntokenizer = T5Tokenizer.from_pretrained(\"/content/t5_cnn_model_base_v2\")\n\n# Save to HuggingFace\nmodel.push_to_hub(\"vlassner01/t5_cnn_model_base_v2\")\ntokenizer.push_to_hub(\"vlassner01/t5_cnn_model_base_v2\")","metadata":{"id":"VMhYxlpzIt6i","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:25:40.644854Z","iopub.execute_input":"2025-05-03T16:25:40.645571Z","iopub.status.idle":"2025-05-03T16:26:33.622265Z","shell.execute_reply.started":"2025-05-03T16:25:40.645547Z","shell.execute_reply":"2025-05-03T16:26:33.621632Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a45c856156ab4a35ade52180320ae348"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a75ac7d6b0d047719e63012206fab873"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bd49ffa163b4d5489aeedf76d622e4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c65f62828e9e44e6b6545b99dc112e32"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/vlassner01/t5_cnn_model_base_v2/commit/448396b176e65c1b8855c9e5a6882ba0f21b2343', commit_message='Upload tokenizer', commit_description='', oid='448396b176e65c1b8855c9e5a6882ba0f21b2343', pr_url=None, repo_url=RepoUrl('https://huggingface.co/vlassner01/t5_cnn_model_base_v2', endpoint='https://huggingface.co', repo_type='model', repo_id='vlassner01/t5_cnn_model_base_v2'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# # Manually upload the file to HuggingFace\n# # File: speice.model wouldn't upload\n\n# from huggingface_hub import Repository\n# from transformers import T5Tokenizer\n\n# # Load and save tokenizer\n# tokenizer = T5Tokenizer.from_pretrained(\"/content/t5_cnn_model_base_v2\")\n# tokenizer.save_pretrained(\"/content/t5_cnn_model_base_v2\")\n\n# # Initialize Hugging Face repo\n# repo = Repository(\n#     local_dir=\"/content/t5_cnn_model_base_v2\",\n#     clone_from=\"vlassner01/t5_cnn_model_base_v2\"\n# )\n\n# # Track and push all files, including spiece.model\n# repo.git_add(auto_lfs_track=True)\n# repo.git_commit(\"Uploading tokenizer with spiece.model\")\n# repo.git_push()\n","metadata":{"id":"zmEsID1yZdWy","trusted":true,"execution":{"iopub.status.busy":"2025-05-03T16:29:29.929685Z","iopub.execute_input":"2025-05-03T16:29:29.930493Z","iopub.status.idle":"2025-05-03T16:29:29.934261Z","shell.execute_reply.started":"2025-05-03T16:29:29.930458Z","shell.execute_reply":"2025-05-03T16:29:29.933495Z"}},"outputs":[],"execution_count":13}]}